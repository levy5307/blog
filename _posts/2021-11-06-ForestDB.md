---
layout: post
title: ForestDB
date: 2021-11-06
Author: levy5307
tags: [论文]
comments: true
toc: true
---

在过去几年中，数据管理应用正在发生巨大的变化。例如Facebook和Twitter等社交应用，其用户数和处理的数据量变的越来越大，并且为了灵活分析，数据正在变得非结构化。不幸的是，由于关系型数据库的扩展性和严格的data model，使得使用关系型数据库变得非常挣扎，由于这个原因，很多公司开始开发NoSQL数据库。

尽管现在对于sharding、data replication、distributed caching有很多中NoSQL技术，而底层的存储引擎却没有什么不同。每个节点都是用KV存储引擎以一种schema-less的形式存储数据，其中key和value都是不定长的。由于kv存储引擎直接与存储设备（HDD或者SSD）交互，所以其吞吐和延迟决定着整个系统的性能。

kv存储引擎的吞吐量和延迟受存储访问时间的限制，而该存储访问时间由两个因素决定：

1. 每次kv操作所访问的block数量。其由索引结构和特征所决定的

2. block访问模式，其由数据是如何读取及写入引擎有关。

目前有两种比较流行的索引结构：B+-tree和LSM-tree。B+-tree是最流行的索引结构，由于其最小化IO操作数量的特性，广泛应用于各种传统的数据库。现代key-value数据库例如BerkeleyDB、Couchbase、InnoDB和MongoDB使用B+-tree。相较于B+-tree，LSM-tree以牺牲读性能来提高写性能，很多最新系统例如LevelDB、RocksDB、SQLite、Cassandra和BigTable使用LSM-tree或者其变种。

尽管这些树型结构目前已经很成功了，***但是相较于固定长度key，当key长度可变时其表现的就很差了***。因为当key长度边长时，如果node大小不变，那么fanout degree将会减少（fanout degree是指一个节点中的key-pointer对数量），因此对于维持相同的数据量，***树的高度则会变高***。另外，如果想要保持相同fanout degree，***那么node大小则会变大***，因此每个node所需要读取或者写入的block数量也会变多。很不幸的，由于平均磁盘访问数量和树的磁盘空间占用，直接受树的高度和节点大小的影响，因此随着key变长，其性能也跟着降级。

为了解决这个问题，BerkeleyDB和LevelDB使用了前缀压缩技术，然而，这很大程度上受到key模式的影响。如果key在key空间随机分布，那么key剩余的未压缩空间仍然很长，因此前缀压缩带来的收益很小。我们需要设计一种更有效的方法来索引变长key。

同时，磁盘访问方式是kv存储设计的另外一个关键因素。update-in-place提供了较高的读性能，但是损失写性能，因此不适合写密集型应用。因此，很多数据库使用append-only或者WAL来顺序写入磁盘block。这种设计可以达到很高的写吞吐，但是却要遭到compaction带来的开销。这种开销的大小与每个索引操作的平均访问block数和索引结构所占用的总体空间密切相关，因为在compaction过程中执行了许多合并操作。

这篇论文主要讲述了ForestDB，一个用于下一代Couchbase的KV存储引擎。为了以高效利用时间和空间的方式检索变长key，提出了Hierarchical B+-tree-based trie（HB+-trie）。为了实现高读取和写入吞吐，以及支持高并发访问，索引更新以append-only的形式写入存储设备中。这同样简化了on-disk结构，以实现MVCC。最后我们实现了ForestDB，其吞吐显著高于LevelDB、RocksDB和Couchstore

## Background

### B+-Tree和Prefix B+-Tree

B+-Tree有两种节点：leaf节点和index节点（non-leaf节点）。并且不像B-Tree和其他的二叉查找树，B+-tree仅仅在leaf节点保存数据，index节点保存指向其child节点的指针。通常一个节点中会保存多于两个的key-value（或者key-pointer），保存的key-value或者key-pointer数量叫做fanout。通常fanout会配置成一个很大的值，这样使得某个节点可以装入一个或者多个blocks。这样可以***减少key-value访问的平均磁盘访问数***。

然而fanout是受到key长度影响的。为了减少这种开销而提出了Prefix B+-Tree。其主要思想是保存key中不同的部分、而非全部key，以提高fanout。在Prefix B+-Tree中，index节点保存可以区分不同child的key的最小前缀，而leaf节点则越过key之间的共同前缀，只保存不同部分。

下图所示为一个B+-Tree及其对应的Prefix B+-Tree:

[](../images/prefix-b+-tree.jpg)

***B+-Tree的优点：*** 减少IO的平均磁盘访问数量。

***B+-Tree的缺点：*** B+-Tree的随机磁盘访问可能会带来较低的性能。尤其当B+-Tree运行较长时间时，其节点将散列分布在磁盘上。

### LSM-Tree

上面讲到B+-Tree由于随机磁盘访问可能会导致性能较低，但是其他树形结构可能也会有类似的问题，因为其不能像B+-Tree一样减少磁盘访问数量。然而我们却通过arrange磁盘写入模式的方式来提高写入性能。这就是LSM-Tree。

在LSM-Tree中，不管是wal append还是compaction的merge操作，都是执行顺序写，因此其写入性能是比B+-Tree强的。然而，读取却可能需要遍历LSM-Tree的多层，所以其读性能是不如B+-Tree的。

注意，当key长度变大时，每层所能存储的key的数量就会变少，因此compaction就会变的频繁。由于compaction会设计大量的磁盘IO，整体的性能就会变低。

### Couchstore

Couchstore是Couchbase的存储引擎，其使用了B+-tree的变种，其将B+-tree改良成了一种append-only的形式。

Couchstore将key均分成了一定数量（该数量是由用户自定义的）的key ranges，每个key range称为一个vBucket，而每个vBucket拥有其自己的DB file。DB file存储其对应的vBucket中的key-value，其中key是一个任意长度的字符串，而value是一个json文档。

DB file保存如下数据：

- 对应vBucket中的key-value，其中key是任意长度的字符串，value是一个json document

- B+-tree。为了检索DB file中的json document，B+-tree保存key及其对应的json document在DB file中的偏移。

前面讲到，Couchstore使用在DB file中追加的方式来实现update操作。下图是一个update操作的示例：

[](../images/update-in-couchstore.jpg)

在图中，A, B, C代表B+-tree节点，而D1, D2, D3代表json document。如果document D1更新了，new document D1'将会追加到DB file中，而非通过修改D1的方式实现。此时，由于document的位置发生了改变，node B将会修改为B'，当然也是通过追加的形式修改。类似的，修改会追加到root节点，即：A'也会追加到DB file中。

***优点：***相比传统B+-tree update-in-place的实现方式，append-only B+-tree可以实现较高的写入吞吐，因为写入是顺序写。同时，由于读取的方式与原始B+-tree一样，所以在这里并没有丧失读性能。

***缺点：***相比传统B+-tree update-in-place的实现方式，其磁盘空间消耗会比较大，因此需要周期的回收stale data占用的磁盘空间。

该回收是通过Compaction流程来实现的。当stale data大小超过一个配置的值时，该compaction将会被触发：

1. 所有有效的document将会移动到一个新的DB file中

2. 当compaction执行完时，old DB file将会被移除。

在compaction执行过程中，所有的写入都会被block住，而读取则不用。另外需要注意，在同一时间，一次只有一个DB file执行compaction操作。

与传统B+-tree一样，当key长度变大时，树的高度将变大。但是其情况更糟，因此append-only的方式，其每次更新的写入数据与树的高度成正比（因为要写入node节点）。这将导致compaction会被触发的更加频繁，因此性能会变得更差。

## ForestDB
