---
layout: post
title: ForestDB
date: 2021-11-06
Author: levy5307
tags: [论文]
comments: true
toc: true
---

在过去几年中，数据管理应用正在发生巨大的变化。例如Facebook和Twitter等社交应用，其用户数和处理的数据量变的越来越大，并且为了灵活分析，数据正在变得非结构化。不幸的是，由于关系型数据库的扩展性和严格的data model，使得使用关系型数据库变得非常挣扎，由于这个原因，很多公司开始开发NoSQL数据库。

尽管现在对于sharding、data replication、distributed caching有很多中NoSQL技术，而底层的存储引擎却没有什么不同。每个节点都是用KV存储引擎以一种schema-less的形式存储数据，其中key和value都是不定长的。由于kv存储引擎直接与存储设备（HDD或者SSD）交互，所以其吞吐和延迟决定着整个系统的性能。

kv存储引擎的吞吐量和延迟受存储访问时间的限制，而该存储访问时间由两个因素决定：

1. 每次kv操作所访问的block数量。其由索引结构和特征所决定的

2. block访问模式，其由数据是如何读取及写入引擎有关。

目前有两种比较流行的索引结构：B+-Tree和LSM-Tree。B+-Tree是最流行的索引结构，由于其最小化IO操作数量的特性，广泛应用于各种传统的数据库。现代key-value数据库例如BerkeleyDB、Couchbase、InnoDB和MongoDB使用B+-Tree。相较于B+-Tree，LSM-Tree以牺牲读性能来提高写性能，很多最新系统例如LevelDB、RocksDB、SQLite、Cassandra和BigTable使用LSM-Tree或者其变种。

尽管这些树型结构目前已经很成功了，***但是相较于固定长度key，当key长度可变时其表现的就很差了***。因为当key长度边长时，如果node大小不变，那么fanout degree将会减少（fanout degree是指一个节点中的key-pointer对数量），因此对于维持相同的数据量，***树的高度则会变高***。另外，如果想要保持相同fanout degree，***那么node大小则会变大***，因此每个node所需要读取或者写入的block数量也会变多。很不幸的，由于平均磁盘访问数量和树的磁盘空间占用，直接受树的高度和节点大小的影响，因此随着key变长，其性能也跟着降级。

为了解决这个问题，BerkeleyDB和LevelDB使用了前缀压缩技术，然而，这很大程度上受到key模式的影响。如果key在key空间随机分布，那么key剩余的未压缩空间仍然很长，因此前缀压缩带来的收益很小。我们需要设计一种更有效的方法来索引变长key。

同时，磁盘访问方式是kv存储设计的另外一个关键因素。update-in-place提供了较高的读性能，但是损失写性能，因此不适合写密集型应用。因此，很多数据库使用append-only或者WAL来顺序写入磁盘block。这种设计可以达到很高的写吞吐，但是却要遭到compaction带来的开销。这种开销的大小与每个索引操作的平均访问block数和索引结构所占用的总体空间密切相关，因为在compaction过程中执行了许多合并操作。

这篇论文主要讲述了ForestDB，一个用于下一代Couchbase的KV存储引擎。为了以高效利用时间和空间的方式检索变长key，提出了Hierarchical B+-Tree-based trie（HB+-trie）。为了实现高读取和写入吞吐，以及支持高并发访问，索引更新以append-only的形式写入存储设备中。这同样简化了on-disk结构，以实现MVCC。最后我们实现了ForestDB，其吞吐显著高于LevelDB、RocksDB和Couchstore

## Background

### B+-Tree和Prefix B+-Tree

B+-Tree有两种节点：leaf节点和index节点（non-leaf节点）。并且不像B-Tree和其他的二叉查找树，B+-Tree仅仅在leaf节点保存数据，index节点保存指向其child节点的指针。通常一个节点中会保存多于两个的key-value（或者key-pointer），保存的key-value或者key-pointer数量叫做fanout。通常fanout会配置成一个很大的值，这样使得某个节点可以装入一个或者多个blocks。这样可以***减少key-value访问的平均磁盘访问数***。

然而fanout是受到key长度影响的。为了减少这种开销而提出了Prefix B+-Tree。其主要思想是保存key中不同的部分、而非全部key，以提高fanout。在Prefix B+-Tree中，index节点保存可以区分不同child的key的最小前缀，而leaf节点则越过key之间的共同前缀，只保存不同部分。

下图所示为一个B+-Tree及其对应的Prefix B+-Tree:

![](../images/prefix-b+-tree.jpg)

***B+-Tree的优点：*** 减少IO的平均磁盘访问数量。

***B+-Tree的缺点：*** B+-Tree的随机磁盘访问可能会带来较低的性能。尤其当B+-Tree运行较长时间时，其节点将散列分布在磁盘上。

### LSM-Tree

上面讲到B+-Tree由于随机磁盘访问可能会导致性能较低，但是其他树形结构可能也会有类似的问题，因为其不能像B+-Tree一样减少磁盘访问数量。然而我们却通过arrange磁盘写入模式的方式来提高写入性能。这就是LSM-Tree。

在LSM-Tree中，不管是wal append还是compaction的merge操作，都是执行顺序写，因此其写入性能是比B+-Tree强的。然而，读取却可能需要遍历LSM-Tree的多层，所以其读性能是不如B+-Tree的。

注意，当key长度变大时，每层所能存储的key的数量就会变少，因此compaction就会变的频繁。由于compaction会设计大量的磁盘IO，整体的性能就会变低。

### Couchstore

Couchstore是Couchbase的存储引擎，其使用了B+-Tree的变种，其将B+-Tree改良成了一种append-only的形式。

Couchstore将key均分成了一定数量（该数量是由用户自定义的）的key ranges，每个key range称为一个vBucket，而每个vBucket拥有其自己的DB file。DB file存储其对应的vBucket中的key-value，其中key是一个任意长度的字符串，而value是一个json文档。

DB file保存如下数据：

- 对应vBucket中的key-value，其中key是任意长度的字符串，value是一个json document

- B+-Tree。为了检索DB file中的json document，B+-Tree保存key及其对应的json document在DB file中的偏移。

前面讲到，Couchstore使用在DB file中追加的方式来实现update操作。下图是一个update操作的示例：

![](../images/update-in-couchstore.jpg)

在图中，A, B, C代表B+-Tree节点，而D1, D2, D3代表json document。如果document D1更新了，new document D1'将会追加到DB file中，而非通过修改D1的方式实现。此时，由于document的位置发生了改变，node B将会修改为B'，当然也是通过追加的形式修改。类似的，修改会追加到root节点，即：A'也会追加到DB file中。

***优点：***相比传统B+-Tree update-in-place的实现方式，append-only B+-Tree可以实现较高的写入吞吐，因为写入是顺序写。同时，由于读取的方式与原始B+-Tree一样，所以在这里并没有丧失读性能。

***缺点：***相比传统B+-Tree update-in-place的实现方式，其磁盘空间消耗会比较大，因此需要周期的回收stale data占用的磁盘空间。

该回收是通过Compaction流程来实现的。当stale data大小超过一个配置的值时，该compaction将会被触发：

1. 所有有效的document将会移动到一个新的DB file中

2. 当compaction执行完时，old DB file将会被移除。

在compaction执行过程中，所有的写入都会被block住，而读取则不用。另外需要注意，在同一时间，一次只有一个DB file执行compaction操作。

与传统B+-Tree一样，当key长度变大时，树的高度将变大。但是其情况更糟，因此append-only的方式，其每次更新的写入数据与树的高度成正比（因为要写入node节点）。这将导致compaction会被触发的更加频繁，因此性能会变得更差。

另外，这种append-only的优化方式有助于实现MVCC。

## ForestDB

ForestDB是为了高效检索变长key而产生的，用来作为Couchstore的替代品。两者之间的架构是类似的，只是有如下两个不同：

1. ForestDB使用了HB+-trie，HB+-trie相比与传统的B+-Tree对变长key更加高效。

2. 通过使用log-structured write buffer，ForestDB具有更高的写入吞吐。log-structured write buffer的原理与LSM-Tree中的write buffer+WAL是类似的，但是并不需要LSM-Tree中write buffer向LSM-Tree的merge操作。

![](../images/forestdb-arch.jpg)

上图是ForestDB的整体架构图。类似Couchstore，每个vBucket对应一个ForestDB实例。每个ForestDB包含一个in-memory write buffer和一个HB+-trie。所有的document update追加在DB file末尾，并在write buffer记录document在磁盘上的位置。当write buffer中entry的数量超过某上线时，其将会被flush进HB+-trie并永久的存储在DB file中。

另外，和Couchstore一样，在ForestDB中会有一个compaction流程，当stale data size超过上线时，其将被触发，所有有效数据将通过compaction迁移至新的DB file。

### HB+-Trie

![](../images/hb-trie-arch.jpg)

上图展示了HB+-trie的逻辑布局。HB+-Trie内所有B+-Tree的leaf节点存储内容分为两种：

1. 其sub-tree的root节点

2. document数据

B+-tree节点和document都是以追加的形式写入DB file，因此这些DB file是互相交织的，并以MVCC形式维护。下图展示如何以MVCC形式在磁盘上存储node节点

![](../images/hb-trie-data-model.jpg)

HB+-Trie将key切割成固定大小的chunk（该chunk大小是可配置的），在检索一个key时，首先通过第一个chunk检索root B+-tree，我们将得到一个offset：

- 如果document存储在这个位置，那么搜索就完成了

- 如果该位置存储的是sub-tree的root地址，将继续使用下一个chunk来检索该sub-tree

持续重复以上检索过程，直到找到目标document。

HB+-Trie有如下几个优点：

- 由于chunk大小是固定的，并且小于key长度，所以HB+-Trie中B+-tree的fanout比原始B+-tree大很多，因此每个B+-tree的高度都小很多。

- key之间的common branch代表共享的common前缀，这部分数据将被skip和压缩，无需重复保存。

在HB+-trie中，只有至少两个branch穿过某tree时，才会为其创建一个sub-tree。并且在HB+-trie中保存的数据将以能够与其他数据区分的、最少的chunk数量来索引（如图中root B+-tree中非指向sub-tree的另外几个指针，只需要一个chunk就可以检索到数据），这样可以避免后续chunk的检索，提高性能。

![](../images/hb+-trie-insertion.jpg)

上图展示了向HB-trie插入数据的示例。有如下几个假设：

- chunk size是一个字节，即一个字符

- 每个三角代表HB+-trie中的一个sub B+-tree

- 每个B+-tree中的文字代表：1. 作用于该tree的chunk number；2. 被skip的共同前缀

(a)展示了初始状态。此时仅仅索引了一个key `aaaa`。尽管在`aaaa`中有4个chunk，该document仍然由第一个chunk `a`在root B+-tree树中索引，因为`aaaa`是由`a`为开头的唯一的key。我们可以确保当input key的第一个chunk是`a`时，与该input key对应的只有一个key，即`aaaa`。***因此对应于后续chunk的sub tree的检索可以避免***。

(b)展示了当`aaab`插入时HB+-trie的状态。此时一个新的sub B+-tree将会被创建，因为`aaab`同样由chunk a为开头。由于`aaaa`和`aaab`之间的最长common prefix是`aaa`，所以新sub-tree使用第四个chunk来检索他们，并在tree中存储`aa`，该`aa`是作为parent tree（即root B+-tree）和sub tree（即新创建的tree）之间的skipped prefix。

(c)展示了当`aabb`插入时HB+-trie的状态。尽管`aabb`以chunk `a`开头，但是其却不能与skipped prefix `aa`匹配，因此一个新的sub tree在root和现存的sub tree之间产生。新的sub tree使用第三个chunk来作为检索，因为`aabb`和其他现在的common prefix `aaa`之间的的common prefix是`aa`。而skipped prefix `a`则存储在新sub tree中，而现存的sub tree中保存的skipped prefix则被清楚掉，因为在这两者之间已经没有skipped prefix了。

![](../images/hb-trie-random.jpg)

从上面的例子中可以看到，当common prefix足够长时，HB+-trie是很有利的。另外，当key的分布很均匀、不存在common prefix，其仍然很有利。上图展示了chunk size是2字节时均匀分布的例子。由于key之间没有共同前缀，因此每个key都可以通过第一个chunk来检索。因此，该HB+-trie仅仅只有一个sub B+-tree。

假设chunk大小是n，并且key是随机均匀分布的，那么将有高达2<sup>n<sup>个key可以在root B+-tree中检索，相比于传统的B+-tree，这将很大程度上减少该index structure锁带来的空间占用（基本差一个数量级）。这种优势主要来自于，HB+-trie是由第一个chunk来检索，而传统B+-tree则需要整个key来进行检索。

***总结：***在以下两个场景中，HB+-trie可以高效减少多余tree tranversal所带来的磁盘访问次数:

1. common prefix足够长

2. 他们的分布是随机的、均匀的，因此没有common prefix

### 避免倾斜

由于trie是非平衡结构，所以在一些特定情况下，HB+-trie是有可能产生倾斜的。下图展示了chunk size是一字节时两种倾斜的例子：

![](../images/hb+-trie-skew.jpg)

(a)中展示的是一个chunk不断重复的例子：b, bb, bbb。随着倾斜的分支变大，磁盘访问的数量也增加了。

(b)中展示的是0/1的双字符排列，即每个chunk只有两种选择：0和1。其fanout远远小于传统B+-tree。并且像图中第二层很多B+-tree都是空的（不指向document数据），所以不论是空间消耗还是磁盘访问，其都不及传统B+-tree

为了解决倾斜的问题，文章里提出了一种优化。

定义***leaf B+-tree***：一个B+-tree没有sub-tree，并且其不是root B+-tree。leaf B+-tree与其他B+-tree的不同在于，其对应的chunk大小是可变的（其他的B+-tree的chunk大小都是固定的）。并且leaf B+-tree对应的chunk是其parent B+-tree对应chunk的剩余后缀。

![](../images/leaf-B+-tree.jpg)

上图展示了leaf B+-tree的组织架构。在(a)中最左边的leaf B+-tree分别使用`aaa`, `aabc`, `abb`索引了`aaaa`, `aaabc`和`aabb`。这样即使我们插入了一个会导致倾斜的key pattern，也不会导致额外的sub-tree创建，因此多余的tree遍历得以避免。

### Leaf B+-Tree Extension

